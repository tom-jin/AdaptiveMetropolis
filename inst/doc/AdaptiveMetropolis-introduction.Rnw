\documentclass[a4paper]{article}
\usepackage{hyperref}
\usepackage{amsmath, amsthm, amssymb}
\usepackage[round]{natbib}
\usepackage{fullpage}
% The line below tells R to use knitr on this.
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Introduction to Adaptive Metropolis}

\title{Adaptive Metropolis Hastings Algorithms}
\author{Tom Jin \and Xiao Yu Lu}

\begin{document}

\maketitle

\begin{abstract}
We present the R package `AdaptiveMetropolis', which implements several adaptive Metropolis Hastings algorithms. 
These adaptive algorithms use various strategies to overcome a lack of knowledge about the target distribution by adapting the proposal distribution with the information inferred from sampling.
\end{abstract}

\section{Introduction}
<<packages, message=FALSE, cache=TRUE>>= 
require(AdaptiveMetropolis) 
@
This package implements a sequence of adaptive Metropolis Hastings algorithms that build on the each other to adapt to more complex distributions and one algorithm to handle nested models.



\subsection{Related Work}
\section{Adaptive MCMC Algorithms}
\subsection{Adaptive Metropolis}
The Adaptive Metropolis algorithm as described by \citet{haario} is the first in a family of Metropolis Hastings samplers that foregoes reversibility in the Markov chain in favour of continuous updates to the proposal distribution.
Adaptation allows for distributions to be explored much quicker than standard Metropolis Hastings with a poorly tuned proposal distribution.

For this algorithm it is essential that a burn in of some length is run to prevent trapping the chain in a lower dimensional subspace.
This happens when the initial jumps are very close together and very correlated causing subsequent samples to be highly correlated until the chain breaks free after several thousand iterations. 
By running a burn in phase of random walk Metropolis Hastings the chain has had the opportunity to explore a little and adapt much faster.

<<AM-code, eval=FALSE, cache=TRUE>>=
data <- AM(function(x) {dnorm(x, 100,100)}, 5000, 0, 1)
plot(data)
@


<<AM-plots, cache=TRUE, echo=FALSE, message=FALSE, fig.height=5>>=
require(MASS)
target <- function(x) {dnorm(x, 100,100)}
n <- 5000
init.state <- 0
init.cov <- 1
burn.in <- 1000

# Init variables
  d <- length(init.state)
  X <- matrix(NA, nrow = n + burn.in + 1, ncol = d)
  X[1, ] <- init.state
  C <- rep(init.cov, n + burn.in + 1)

  # Vanilla Metropolis phase. (Burn in)
  if (burn.in > 0)
    for(i in seq(1, burn.in)) {
      # Propose a move
      Y <- mvrnorm(n = 1, mu = X[i, ], Sigma = init.cov)
      
      # Calculate acceptance ratio
      a <- min(1, target(Y)/target(X[i, ])) 
      
      if(runif(1) < a) 
        X[i+1, ] <- Y # Accept
      else
        X[i+1, ] <- X[i, ] # Reject
    }
  
  # Initialise covariance
  if(burn.in > 1) {
    C[burn.in+1] <- var(X[seq(1, burn.in), ])
  }

  # Adapted Metropolis phase
  for(i in seq(1, n) + burn.in) {
    # Propose a move
    Y <- mvrnorm(n = 1, mu = X[i, ], Sigma = C[i])

    # Calculate acceptance ratio
    a <- min(1, target(Y)/target(X[i, ])) 
    
    if(runif(1) < a) 
      X[i+1, ] <- Y # Accept
    else
      X[i+1, ] <- X[i, ] # Reject
    
    Xbarold <- .colMeans(X[1:i, ], i, d)
    Xbarnew <- .colMeans(X[seq(1, i+1), ], i+1, d)
    
    # Update covariance
    C[i+1] <- ((i-1)/i) * C[i] + 
      (2.4^2)/(d*i) * ((i * Xbarold %*% t(Xbarold))
                       - ((i + 1) * Xbarnew %*% t(Xbarnew)) 
                       + (X[i+1, ] %*% t(X[i+1, ])) 
                       + .Machine$double.eps*diag(d))
  }
#par(mfrow=c(1,2))
@
<<AM-trace, cache=TRUE, echo=FALSE, message=FALSE, fig.height=5>>=
plot(X, xlab = "Sample", ylab = "Value")
abline(v = 1000)
@

In this example run no knowledge of the target distribution is assumed and the proposal distribution is set up as a univariate Gaussian with mean 0 and variance 1 but unknown to the sampler target distributions is a univariate Gaussian with mean 100 and variance 100. The sampler will have to find the mode and adapt its variance in order to explore it effectively.

Normally the burn in samples are discarded due to poor mixing. They can be observed on the left of the vertical line at sample 1000. As soon as the adaptation begins the varaince exploads and slowly tends towards variance 50000. Within a small number of iterations much of the mode has been explored.

<<AM-variance, cache=TRUE, echo=FALSE, message=FALSE, fig.height=5>>=
plot(C, xlab = "Sample", ylab = "Variance")
abline(v = 1000)
@

\subsection{Adaptive Scaling Within Adaptive Metropolis}
An adaptive scaling with adaptive Metropolis implementation based on \citet{andrieu} scales its variance matrix in order to achieve a 0.23 acceptance ratio.

<<ASWAM, cache=TRUE>>=
library(mvtnorm)
data <- ASWAM(function(x) {dmvnorm(x, rep(10, 8), 10*diag(8))}, 
              1000, rep(0, 8), diag(8))
pairs(data)
@

\subsection{Robust Adaptive Metropolis}

\subsubsection{Introduction}

The Robust Adaptive Metropolis Algorithm (RAM) is an extension of the normal Metropoliis-Hasting Algorithm, where it estimates the shape of the target distribution and coerces the acceptance rate simultaneously, by adapting the proposal covariance structuring according to the data one has simulated. The adaptation rule is computationally simple adding no extra cost compared with the AM algorithm.

\subsubsection{Algorithm}

Suppose the proposal density $q$ is spherically symmetric. Let $S \in \mathbf{R}^{d \times d}$ be a lower-diagonal matrix with positive diagonal elements, $\{\eta_n\}$ be a step size sequence decaying to zero, $\alpha_*$ be the target mean acceptance probability. The RAM algorithm is defined as:

for n = 1,2,..., iterate:

\begin{itemize}

\item compute $X := X_{n-1} + S_{n-1}U_n$, where $U_n \sim q$ is an independent random vector.
\item with probability $\alpha_n : = \min(1,\pi(X)/\pi(X_{n-1}))$, set $X_n = X$ and $X_n = X_{n-1}$ otherwise.
\item update $S_n$ through the equation: \\

\hspace{3cm}  $S_nS_n^T = S_{n-1}(I + \eta_n(\alpha_n - \alpha_*)\frac{U_nU_n^T}{\|U_n\|^2})S_{n-1}^T$,  
            
where $I \in \mathbf{R}^{d \times d}$ stands for the identity matrix.
\end{itemize}

\noindent Note there is a unique $S_n$ satisfying 3 above, since the right hand side is symmetric and positive definite.


\subsubsection{Inputs and outputs}
the RAM algorithm takes 4 inputs including the target distribution $\pi$ (target), the number of iterations (N), the initial state (init.state) and the initial covariance structure for the MVN proposal (init.cov). The output is of a matrix form, recording the simulated path.


\subsubsection{Example}
We implement a toy example here. We set the initial state to be the origin in a 3-dimensional space, and the target distribution is a standard MVN with identity covariance structure, and we iterate for 10,000 times:

<<chunk1, cache=TRUE>>=
library(mvtnorm)  
res <- RAM(function(x) {dmvnorm(x, rep(0, 3), diag(3))}, 10000, 
           c(0,0,0), diag(3))
@

The traceplot of the 1st component and the histogram can be found below:

<<chunk2, cache=TRUE, echo=FALSE, fig.height=3>>=
plot(res[,1],type = "l",xlab="iteration",main="Traceplot")
@

<<chunk2a, cache=TRUE, echo=FALSE, fig.height=3>>=
hist(res[,1],main="Histogram")
@


\subsection{Adaptive Metropolis-Within-Gibbs}


\subsubsection{introduction}
We implement one of the examples in \citet{roberts}, namely the Metropolis Within Gibbs Algorithm (MWGA), which applies the algorithm to a hierarchical model:

\begin{itemize}
\item Each observation $Y_{ij} \sim N(\theta_i,V)$ for $[1 \leq j \leq r_i]$.
\item Each parameter $\theta_i \sim Cauchy(\mu,A)$ for $[1 \leq i \leq K]$.
\item $\mu, A, V$ follow $N(0,1), IG(1,1), IG(1,1)$ respectively.
\end{itemize}
\subsubsection{Algorithm}
The AMWG algorithm extends the usual Gibbs sampler, so that we update each parameter using a random walk proposal, and accept or reject it with certain acceptance probability, which is adapted in each iteration according to the simulated path one has already obtained at that time.

In each $n^{th}$ "batch", we append $A, V, \mu$ to $\theta$ and update each of the (K+3) parameters in turn by proposing a $N(0,\sigma_{i,n}^2)$ increment, where $sigma$ is adjusted according to the following rule: \\
\begin{itemize}
\item Set $\sigma_{i,n} = 1$ when $n<50$ for $1 \leq i \leq K+3$.
\item Update $\log(\sigma_{i,n}) = \log(\sigma_{i,n}) + \delta(n)$ \;\;\ if proportion(accepted $\theta_i$s) $\geq 0.44$; \\ 
\hspace*{2.9cm} $\log(\sigma_{i,n}) - \delta(n)$ \;\; if proportion(accepted $\theta_i$s) $\leq 0.44$. \\

where $\delta(n)$ decays to 0 and is chosen to be $\min(0.005,n^{-1/2}).$
\end{itemize}

\subsubsection{Inputs and Outputs}
AMWG takes three inputs including whether the proposal variance is adapted (adapt), the number of iterations(N) and the number of parameters $\theta$(K). The output contains a list of the parameter ($\theta, A, V, \mu$) paths and the acceptance/rejection paths.

\subsubsection{Example}

We can compare the adapted and unadapted version of the algorithm with $N=1000$ and $K=500$:
<<chunk3, cache=TRUE>>=
out_unadapt <- AMWG(adapt = 0, 1000, 500)
theta_unadapt <- out_unadapt[[1]]
acceptance_unadapt <- out_unadapt[[2]]
out_adapt <- AMWG(adapt = 1, 1000, 500)
theta_adapt <- out_adapt[[1]]
acceptance_adapt <- out_adapt[[2]]
@

<<chunk, cache=TRUE, echo=FALSE, fig.height=5>>=
plot(theta_unadapt[,1],type="l",xlab="number of batch",ylab=expression(theta[1]),main=expression('traceplots of '*theta[1]*' using Metropolis within Gibbs'))
lines(theta_adapt[,1],col=2)
legend("topright",c("unadapted","adapted"),col=c(1,2),lwd=c(2,2))
@

The histograms of the proportion of accepted parameters until the $N^{th}$ batch are displayed here:
<<chunk4, cache=TRUE, echo=FALSE, fig.height=5>>=
par(mfrow=c(2,1))
hist(colMeans(acceptance_unadapt),main="histogram of acceptance ratio for K+3 parameters (unadapted)",xlab="",breaks=50)
hist(colMeans(acceptance_adapt),main="histogram of acceptance ratio for K+3 parameters (adapted)",xlab="",breaks=50)
@

It can be seen that AMWG pulls the acceptance ratio towards 0.44, which is optimal in many settings.

\subsection{Comparison}
We compare the three generic algorithms(AM, ASWAM, RAM) here using a 2-dimensional student distribution with degrees of freedom equal to 1. The simulated patterns are displayed below:

<<echo=FALSE, cache=TRUE, fig.height=3>>=
library(mnormt)
N=10000
init.state = rnorm(2)    #random initialization
d = length(init.state)
init.cov = diag(d)
mean = c(1,2)
sigma = matrix(c(0.2,0.1,0.1,0.8),2,2)
target = function(x) {
  return(dmt(t(x),mean,sigma,1))
}

resAM <- AM(target, N, init.state, init.cov)
resASWAM <- ASWAM(target, N, init.state, init.cov)
resRAM <- RAM(target, N, init.state, init.cov)

par(mfrow=c(1,3))
par(pty="s")
plot(resAM[,1],resAM[,2])
plot(resASWAM[,1],resASWAM[,2])
plot(resRAM[,1],resRAM[,2])
@

It seems that in this case, AM and RAM have better performance compared with ASWAM.

\section{Testing}

This package includes a suite of unit tests written in testthat to validate the functions AM, ASWAM and RAM. 
The tests verify that all of the samplers produce sane results when run to sample from some common distributions in one and multiple dimensions.
The tests also verify the the internal parameter validation function is correctly rejecting invalid parameters.

The testing of this package is integrated with \href{https://travis-ci.org/}{Travis CI}. The continuous integration platform builds and tests the package as code is committed to the package's repository. 

\section{Future Work}
The looped nature of these algorithms would potentially benefit from an implementation using \href{http://www.rcpp.org/}{Rcpp} to achieve near native performance with for loops.
However as long as these samplers accept arbitrary density functions within R there will be a performance penalty when evaluating these densities from within C++. This penalty will remain even if the density itself is a native C/C++/Fortran function as a call will have to be made from C++ to R and then to another compiled language.

Adaptive Metropolis Hastings algorithms do not solve the issues caused by multimodal distributions.
If the modes of a target distribution are far enough apart it is possible that a chain never finds the other modes and adapts itself only to the mode it has found.

\bibliographystyle{plainnat}
\bibliography{adaptive}

\end{document}